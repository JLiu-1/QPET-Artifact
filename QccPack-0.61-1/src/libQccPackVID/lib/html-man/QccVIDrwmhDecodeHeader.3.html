<!-- manual page source format generated by PolyglotMan v3.0.4, -->
<!-- available via anonymous ftp from ftp.cs.berkeley.edu:/ucb/people/phelps/tcltk/rman.tar.Z -->

<HTML>
<HEAD>
<TITLE>QccVIDrwmhDecodeHeader.3</TITLE>
</HEAD>
<BODY>
<A HREF="#toc">Table of Contents</A><P>
 
<H2><A NAME="sect0" HREF="#toc0">NAME </A></H2>
QccVIDrwmhEncode, QccVIDrwmhDecode - encode/decode an image sequence 
using the RWMH algorithm  
<H2><A NAME="sect1" HREF="#toc1">SYNOPSIS </A></H2>
<B>#include "libQccPack.h"</B>  <P>
<B>int QccVIDrwmhEncode(QccIMGImageSequence 
*</B><I>image_sequence</I><B>, const QccFilter *</B><I>filter1</I><B>, const QccFilter *</B><I>filter2</I><B>, const 
QccFilter *</B><I>filter3</I><B>, int </B><I>subpixel_accuracy</I><B>, int </B><I>blocksize</I><B>, QccBitBuffer 
*</B><I>output_buffer</I><B>, int </B><I>num_levels</I><B>, int </B><I>target_bit_cnt</I><B>, const QccWAVWavelet 
*</B><I>wavelet</I><B> const QccString </B><I>mv_filename</I><B>, int </B><I>read_motion_vectors</I><B>, int </B><I>quiet</I><B>);</B> 
 <P>
<B>int QccVIDrwmhDecodeHeader(QccBitBuffer *</B><I>input_buffer</I><B>, int *</B><I>num_rows</I><B>, 
int *</B><I>num_cols</I><B>, int *</B><I>start_frame_num</I><B>, int *</B><I>end_frame_num</I><B>, int *</B><I>num_levels</I><B>, 
int *</B><I>blocksize</I><B>, int *</B><I>target_bit_cnt</I><B>);</B>  <P>
<B>int QccVIDrwmhDecode(QccIMGImageSequence 
*</B><I>image_sequence</I><B>, const QccFilter *</B><I>filter1</I><B>, const QccFilter *</B><I>filter2</I><B>, const 
QccFilter *</B><I>filter3</I><B>, int </B><I>subpixel_accuracy</I><B>, int </B><I>blocksize</I><B>, QccBitBuffer 
*</B><I>input_buffer</I><B>, int </B><I>target_bit_cnt</I><B>, int </B><I>num_levels</I><B>, const QccWAVWavelet 
*</B><I>wavelet</I><B>, const QccString </B><I>mv_filename</I><B>, int </B><I>quiet</I><B>);</B>  
<H2><A NAME="sect2" HREF="#toc2">DESCRIPTION </A></H2>
 
<H3><A NAME="sect3" HREF="#toc3">Encoding 
</A></H3>
<P>
<B>QccVIDrwmhEncode()</B> encodes an <I>image_sequence</I> using the redundant-wavelet-multihypothesis 
(RWMH) video-coding algorithm by Cui <I>et al</I>. Essentially, the RWMH algorithm 
involves traditional block-based motion estimation and motion compensation 
wherein the redundant phases of the RDWT of the reference frame are used 
to provide a multihypothesis estimate of motion based on the diversity 
of the transform phases; see "ALGORITHM" below for greater detail. <P>
<I>image_sequence</I> 
is the image sequence to be coded and should indicate a collection of 
grayscale images of the same size stored as separate, numbered files; 
the filename indicated by <I>image_sequence</I> must contain one  <B><A HREF="printf.3.html">printf</B>(3)</A>
-style 
numerical descriptor which will then be filled in the current frame number 
(e.g., football.%03d.pgm will become football.000.pgm, football.001.pgm, etc.; 
see <B><A HREF="QccPackIMG.3.html">QccPackIMG</B>(3)</A>
). Each frame of <I>image_sequence</I> must have a size which 
is an integer multiple of <I>blocksize</I> both horizontally and vertically. Both 
the <I>start_frame_num</I> and <I>end_frame_num</I> fields of <I>image_sequence</I> should 
indicate the desired starting and stopping frames, respectively, for the 
encoding; these should either be set manually or via a call to <B><A HREF="QccIMGImageSequenceFindFrameNums.3.html">QccIMGImageSequenceFindFrameNums</B>(3)</A>
 
prior to calling <B>QccVIDrwmhEncode()</B>. <P>
<I>filter1</I>, <I>filter2</I>, and <I>filter3</I> are 
interpolation filters for supporting the subpixel accuracy specified by 
<I>subpixel_accuracy</I> which can be one of <B>QCCVID_ME_FULLPIXEL</B>, <B>QCCVID_ME_HALFPIXEL</B>, 
<B>QCCVID_ME_QUARTERPIXEL</B>, or  <B>QCCVID_ME_EIGHTHPIXEL</B>, indicating full-, half-, 
quarter-, or eighth-pixel accuracy, respectively. See "SUBPIXEL ACCURACY" 
below. <P>
<I>block_size</I> is the size of square blocks to use in the RWMH algorithm. 
<P>
<I>output_buffer</I> is the output bitstream which must be of <B>QCCBITBUFFER_OUTPUT</B> 
type and opened via a prior call to <B><A HREF="QccBitBufferStart.3.html">QccBitBufferStart</B>(3)</A>
. <P>
<I>num_levels</I> gives 
the number of levels of dyadic wavelet decomposition to perform, and <I>wavelet</I> 
is the wavelet to use for decomposition, in all the RDWTs in the RWMH 
algorithm. <P>
The RWMH encoder uses a cross-scale distortion measure to determine 
a motion vector for a "set" of spatially co-located blocks in the RDWT 
of the current frame. The number of transform scales used in this cross-scale 
is <I>num_levels</I>. The result of this motion-estimation search is an "all-phase" 
motion vector for the set of blocks. <P>
When encoding the current frame, <B>QccVIDrwmhEncode()</B> 
will first output to <I>output_buffer</I> all the motion vectors for the frame, 
and then an embedded intraframe encoding of the motion-compensated residual. 
<I>target_bit_cnt</I> is the desired number of bits to output for each frame, 
including motion vectors and motion-compensated residual. After <I>target_num_bits</I> 
have been produced for the current frame, <B>QccVIDrwmhEncode()</B> will call 
<B><A HREF="QccBitBufferFlush.3.html">QccBitBufferFlush</B>(3)</A>
 to flush the bit-buffer contents to the output bitstream. 
Encoding of the next frame starts on the next byte boundary of the output 
bitstream. <P>
<I>mv_filename</I> gives the filename for files of motion vectors. If 
<I>read_motion_vectors</I> is <B>FALSE</B>, then the motion-vectors are written to <I>mv_filename</I> 
via <B><A HREF="QccVIDMotionVectorsWriteFile.3.html">QccVIDMotionVectorsWriteFile</B>(3)</A>
. <I>mv_filename</I> should have a <B><A HREF="printf.3.html">printf</B>(3)</A>
-style 
numerical descriptor which will then be filled in with the current frame 
number before writing, so that motion vectors are separated into multiple 
files, one file per frame. On the other hand, if <I>read_motion_vectors</I> is 
<B>TRUE</B>, then motion vectors are read from <I>mv_filename</I> via <B><A HREF="QccVIDMotionVectorsReadFile.3.html">QccVIDMotionVectorsReadFile</B>(3)</A>
, 
in which case <B>QccVIDrwmhEncode()</B> performs no motion estimation itself, 
using simply the motion vectors read from the files for coding. <P>
If <I>quiet</I> 
= 0, <B>QccVIDrwmhEncode()</B> will print to stdout a number of statistics concerning 
each frame as it is encoding. If <I>quiet</I> = 1, this verbose output is suppressed. 
 
<H3><A NAME="sect4" HREF="#toc4">Decoding </A></H3>
<B>QccVIDrwmhDecodeHeader()</B> decodes the header information in a 
bitstream produced by <B>QccVIDrwmhEncode()</B>. The input bitstream is <I>input_buffer</I> 
which must be of <I>QCCBITBUFFER_INPUT</I> type and open via a prior call to 
<B><A HREF="QccBitBufferStart.3.html">QccBitBufferStart</B>(3)</A>
. The header information is returned in <I>num_rows</I> (vertical 
size of image-sequence frames), <I>num_cols</I> (horizontal size of image-sequence 
frames), <I>start_frame_num</I> (number of the first frame of the sequence), 
<I>end_frame_num</I> (number of the last frame of the sequence), <I>num_levels</I> (number 
of wavelet-transform levels), and <I>target_bit_cnt</I> (number of bits encoded 
for each frame). <P>
<B>QccVIDrwmhDecode()</B> decodes the bitstream <I>input_buffer</I>, 
reconstructing each image of the output image sequence and writing it 
to a separate, numbered grayscale-image file. The filename denoted by <I>image_sequence</I> 
must contain one <B><A HREF="printf.3.html">printf</B>(3)</A>
-style numerical descriptor which is filled in 
with the number of the current frame being decoded. The bitstream must 
already have had its header read by a prior call to <B>QccVIDrwmhDecodeHeader()</B> 
(i.e., you call <B>QccVIDrwmhDecodeHeader()</B> first and then <B>QccVIDrwmhDecode()</B>). 
If <I>quiet</I> = 0, then <B>QccVIDrwmhDecode()</B> prints a brief message to stdout 
after decoding each frame; if <I>quiet</I> = 1, then this message is suppressed. 
<P>
<I>filter1</I>, <I>filter2</I>, and <I>filter3</I> are interpolation filters for supporting 
the subpixel accuracy specified by <I>subpixel_accuracy</I> which can be one 
of <B>QCCVID_ME_FULLPIXEL</B>, <B>QCCVID_ME_HALFPIXEL</B>, <B>QCCVID_ME_QUARTERPIXEL</B>, or 
 <B>QCCVID_ME_EIGHTHPIXEL</B>, indicating full-, half-, quarter-, or eighth-pixel 
accuracy, respectively. See "SUBPIXEL ACCURACY" below. <P>
<I>mv_filename</I> gives 
the name of files of motion vectors. If <I>mv_filename</I> is <B>NULL</B>, then <B>QccVIDrwmhDecode()</B> 
simply decodes the motion vectors to use in decoding from the bitstream 
(the usual state of affairs). On the other hand, if <I>mv_filename</I> is not 
<B>NULL</B>, then the motion vectors stored in the bitstream are ignored and, 
rather, the motion vectors are read from <I>mv_filename</I> instead. <I>mv_filename</I> 
should have a <B><A HREF="printf.3.html">printf</B>(3)</A>
-style numerical descriptor which will then be filled 
in with the current frame number before reading via <B><A HREF="QccVIDMotionVectorsReadFile.3.html">QccVIDMotionVectorsReadFile</B>(3)</A>
. 
 
<H2><A NAME="sect5" HREF="#toc5">ALGORITHM </A></H2>
Multihypothesis motion compensation (MHMC) forms a prediction 
in the current frame as a combination of multiple predictions in  an effort 
to combat the uncertainty inherent in the motion-estimation (ME)  process. 
A number of multihypothesis  techniques for motion compensation (MC) have 
been proposed. One approach to MHMC is to implement multihypothesis prediction 
in  the spatial dimensions; i.e., the predictions are culled from  spatially 
distinct locations in the reference frame. Included in this class  of MHMC 
would be subpixel-accurate MC and overlapped block motion  compensation 
(OBMC). Another approach is to deploy MHMC in the  temporal dimension by 
choosing predictions from multiple  reference frames. Examples of this 
class of MHMC are bidirectional prediction  (B-frames) and long-term-memory 
motion  compensation (LTMMC). Cui <I>et al</I>. introduced a new class of MHMC 
by extending the  multihypothesis-prediction concept into the transform 
domain. Specifically, Cui <I>et al</I>. performed ME/MC in the domain of a redundant, 
or overcomplete, wavelet  transform, and used multiple predictions that 
were diverse in transform  phase. The term redundant-wavelet multihypothesis 
(RWMH) was coined to  describe this approach to phase-diversity multihypothesis. 
<P>
In the RWMH approach, both the current and reference frames are transformed 
into RDWT coefficients, and both ME and MC take place in this RDWT domain. 
However, before calculating the residual frame, the motion-compensated 
frame is mapped from the RDWT domain back to the spatial domain via a 
multiple-phase inverse RDWT, and the residual is calculated and coded in 
the original spatial domain. <P>
Intuitively, we observe that each of the critically 
sampled DWTs within an  RDWT will "view" motion from a different perspective. 
Consequently, if motion  is predicted in the RDWT domain, the multiple-phase 
inverse RDWT converts these multiple predictions into a single  multihypothesis 
prediction in the spatial domain. Cui <I>et al</I>. present an analytic derivation 
that substantiates this intuition by quantifying the performance gain 
of RWMH over single-phase  prediction. Key to this analysis is that noise 
in the RDWT domain undergoes a  substantial reduction in variance when 
the multiple-phase inverse RDWT is  applied due to the well-known fact that 
this pseudo-inverse contains a  projection onto the range space of the 
forward transform. Consequently, noise  not captured by the motion model 
is greatly reduced in an RWMH system,  leading to substantial reduction 
in the prediction-residual variance and  higher coding efficiency. <P>
We note 
that Cui <I>et al</I>. have improved the performance of RWMH by combining it with 
other, more traditional forms of multihypothesis, e.g., OBMC. In the present 
implementation of RWMH, these enhancements have not (yet) been implemented. 
 
<H2><A NAME="sect6" HREF="#toc6">SUBPIXEL ACCURACY </A></H2>
Due to the linearity of the RDWT, it is possible to 
implement subpixel interpolation in the RDWT domain in a manner similar 
to as done in the spatial domain to support traditional subpixel accuracy. 
 Specifically, one simply interpolates each subband of the RDWT to subpixel 
accuracy independently. <B>QccVIDrwmhEncode()</B> and <B>QccVIDrwmhDecode()</B> both 
call <B><A HREF="QccVIDMotionEstimationCreateReferenceFrame.3.html">QccVIDMotionEstimationCreateReferenceFrame</B>(3)</A>
 for each subband of 
the RDWT of the reference frame to interpolate the subband to the accuracy 
specified by <I>subpixel_accuracy</I>. The filters <I>filter1</I>, <I>filter2</I>, and <I>filter3</I> 
are passed to <B><A HREF="QccVIDMotionEstimationCreateReferenceFrame.3.html">QccVIDMotionEstimationCreateReferenceFrame</B>(3)</A>
 to control 
whether filtered interpolation or bilinear interpolation is performed 
at each step of the subpixel interpolation. See <B><A HREF="QccVIDMotionEstimationCreateReferenceFrame.3.html">QccVIDMotionEstimationCreateReferenceFrame</B>(3)</A>
 
for more detail.  
<H2><A NAME="sect7" HREF="#toc7">SEE ALSO </A></H2>
<B><A HREF="rwmhencode.1.html">rwmhencode</B>(1)</A>
, <B><A HREF="rwmhdecode.1.html">rwmhdecode</B>(1)</A>
, <B><A HREF="QccWAVWaveletRedundantDWT2D.3.html">QccWAVWaveletRedundantDWT2D</B>(3)</A>
, 
<B><A HREF="QccWAVWaveletRedundantDWT2D.3.html">QccWAVWaveletRedundantDWT2D</B>(3)</A>
, <B><A HREF="QccVIDMotionVectorsReadFile.3.html">QccVIDMotionVectorsReadFile</B>(3)</A>
, <B><A HREF="QccVIDMotionVectorsWriteFile.3.html">QccVIDMotionVectorsWriteFile</B>(3)</A>
, 
<B><A HREF="QccVIDMotionEstimationCreateReferenceFrame.3.html">QccVIDMotionEstimationCreateReferenceFrame</B>(3)</A>
, <B><A HREF="QccSPIHTEncode.3.html">QccSPIHTEncode</B>(3)</A>
, <B><A HREF="QccPackVID.3.html">QccPackVID</B>(3)</A>
, 
<B><A HREF="QccPackSPIHT.3.html">QccPackSPIHT</B>(3)</A>
, <B><A HREF="QccPackWAV.3.html">QccPackWAV</B>(3)</A>
, <B><A HREF="QccPackIMG.3.html">QccPackIMG</B>(3)</A>
, <B><A HREF="QccPack.3.html">QccPack</B>(3)</A>
 <P>
 S. Cui, Y. Wang, 
and J. E. Fowler, "Motion Compensation Via Redundant-Wavelet Multihypothesis," 
<I>IEEE Transactions on Image Processing</I>, submitted March 2004. Revised February 
2005. <P>
 S. Cui, Y. Wang, and J. E. Fowler, "Multihypothesis Motion Compensation 
in the Redundant Wavelet Domain," in <I>Proceedings of the International 
Conference on Image Processing</I>, Barcelona, Spain, September 2003, vol. 
2, pp. 53-56. <P>
 H.-W. Park and H.-S. Kim, "Motion Estimation Using Lowband-Shift 
Method for Wavelet-Based Moving-Picture Coding," <I>IEEE Transactions on Image 
Processing</I>, vol. 9, no. 4, pp. 577-587, April 2000. <P>
  
<H2><A NAME="sect8" HREF="#toc8">AUTHOR </A></H2>
Written by Joe 
Boettcher &lt;jbb15@msstate.edu&gt; based on the originally developed algorithm 
and code by Suxia Cui. <P>
 Copyright (C) 1997-2016  James E. Fowler <P>
  <P>

<HR><P>
<A NAME="toc"><B>Table of Contents</B></A><P>
<UL>
<LI><A NAME="toc0" HREF="#sect0">NAME</A></LI>
<LI><A NAME="toc1" HREF="#sect1">SYNOPSIS</A></LI>
<LI><A NAME="toc2" HREF="#sect2">DESCRIPTION</A></LI>
<UL>
<LI><A NAME="toc3" HREF="#sect3">Encoding</A></LI>
<LI><A NAME="toc4" HREF="#sect4">Decoding</A></LI>
</UL>
<LI><A NAME="toc5" HREF="#sect5">ALGORITHM</A></LI>
<LI><A NAME="toc6" HREF="#sect6">SUBPIXEL ACCURACY</A></LI>
<LI><A NAME="toc7" HREF="#sect7">SEE ALSO</A></LI>
<LI><A NAME="toc8" HREF="#sect8">AUTHOR</A></LI>
</UL>
<p><br><br><a href="http://sourceforge.net/projects/qccpack"><img src="http://sflogo.sourceforge.net/sflogo.php?group_id=5992&type=6" width="210" height="62" border="0" alt="Get QccPack at SourceForge.net. Fast, secure and Free Open Source software downloads" /></a></body></html>
